{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "sample_pdf = open(\"./mast.pdf\",'rb')\n",
    "pdfdoc  = PyPDF2.PdfFileReader(sample_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_basic_validation',\n",
       " '_build_destination',\n",
       " '_build_field',\n",
       " '_build_outline_item',\n",
       " '_check_kids',\n",
       " '_find_eof_marker',\n",
       " '_find_startxref_pos',\n",
       " '_flatten',\n",
       " '_get_indirect_object',\n",
       " '_get_named_destinations',\n",
       " '_get_num_pages',\n",
       " '_get_object_from_stream',\n",
       " '_get_outline',\n",
       " '_get_page',\n",
       " '_get_page_number_by_indirect',\n",
       " '_get_xref_issues',\n",
       " '_pairs',\n",
       " '_read_pdf15_xref_stream',\n",
       " '_read_standard_xref_table',\n",
       " '_read_xref',\n",
       " '_read_xref_other_error',\n",
       " '_read_xref_subsections',\n",
       " '_read_xref_tables_and_trailers',\n",
       " '_rebuild_xref_table',\n",
       " '_write_field',\n",
       " 'cacheGetIndirectObject',\n",
       " 'cacheIndirectObject',\n",
       " 'cache_get_indirect_object',\n",
       " 'cache_indirect_object',\n",
       " 'decode_permissions',\n",
       " 'decrypt',\n",
       " 'documentInfo',\n",
       " 'getDestinationPageNumber',\n",
       " 'getDocumentInfo',\n",
       " 'getFields',\n",
       " 'getFormTextFields',\n",
       " 'getIsEncrypted',\n",
       " 'getNamedDestinations',\n",
       " 'getNumPages',\n",
       " 'getObject',\n",
       " 'getOutlines',\n",
       " 'getPage',\n",
       " 'getPageLayout',\n",
       " 'getPageMode',\n",
       " 'getPageNumber',\n",
       " 'getXmpMetadata',\n",
       " 'get_destination_page_number',\n",
       " 'get_fields',\n",
       " 'get_form_text_fields',\n",
       " 'get_object',\n",
       " 'get_page_number',\n",
       " 'isEncrypted',\n",
       " 'is_encrypted',\n",
       " 'metadata',\n",
       " 'namedDestinations',\n",
       " 'named_destinations',\n",
       " 'numPages',\n",
       " 'outline',\n",
       " 'outlines',\n",
       " 'pageLayout',\n",
       " 'pageMode',\n",
       " 'page_layout',\n",
       " 'page_mode',\n",
       " 'pages',\n",
       " 'pdf_header',\n",
       " 'read',\n",
       " 'readNextEndLine',\n",
       " 'readObjectHeader',\n",
       " 'read_next_end_line',\n",
       " 'read_object_header',\n",
       " 'xfa',\n",
       " 'xmpMetadata',\n",
       " 'xmp_metadata']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(PyPDF2.PdfFileReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Author': '',\n",
       " '/CreationDate': 'D:20221103004837Z',\n",
       " '/Creator': 'LaTeX with hyperref',\n",
       " '/Keywords': '',\n",
       " '/ModDate': 'D:20221103004837Z',\n",
       " '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       " '/Producer': 'pdfTeX-1.40.21',\n",
       " '/Subject': '',\n",
       " '/Title': '',\n",
       " '/Trapped': '/False'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfdoc.documentInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfdoc.getNumPages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TVLT: Textless Vision-Language Transformer\\nZineng Tang\\x03Jaemin Cho\\x03Yixin Nie\\x03Mohit Bansal\\nUNC Chapel Hill\\n{terran, jmincho, yixin1, mbansal}@cs.unc.edu\\nAbstract\\nIn this work, we present the Textless Vision-Language Transformer (TVLT), where\\nhomogeneous transformer blocks take raw visual and audio inputs for vision-and-\\nlanguage representation learning with minimal modality-speciﬁc design, and do\\nnot use text-speciﬁc modules such as tokenization or automatic speech recognition\\n(ASR). TVLT is trained by reconstructing masked patches of continuous video\\nframes and audio spectrograms (masked autoencoding) and contrastive modeling\\nto align video and audio. TVLT attains performance comparable to its text-based\\ncounterpart on various multimodal tasks, such as visual question answering, image\\nretrieval, video retrieval, and multimodal sentiment analysis, with 28x faster infer-\\nence speed and only 1/3 of the parameters. Our ﬁndings suggest the possibility\\nof learning compact and efﬁcient visual-linguistic representations from low-level\\nvisual and audio signals without assuming the prior existence of text.1\\n1 Introduction\\nHumans perceive and learn the external world through signals from multiple modalities. To embody\\nsuch human learning in machines, substantial research efforts are dedicated to developing vision-\\nand-language (VL) models that can understand the joint semantics between visual and linguistic\\nmodalities and solve tasks such as visual question answering [ 4]. Although most such VL models use\\nwritten language rather than spoken language as the main verbal communication channel, the default\\ncommunication modality among humans has been speech, since circa 100,000 BCE [ 78]. Written\\nlanguage is relatively recent; cuneiform script, the earliest writing system, was developed circa 3,200\\nBCE [ 65]. Moreover, we have witnessed an increasing usage of AI models in real-world products\\nsuch as virtual assistants and smart speakers [ 40], where perception-level signals such as video and\\naudio are the natural form of input. Intuitively, direct modeling of such signals will potentially yield\\nmore compact and efﬁcient representations.\\nTransformers [ 81] have recently achieved great success in vision-language representation learning [ 76;\\n10;48;74;87;86] by using text-based modules [ 15] on text-annotated images or videos. However,\\nit is non-trivial to learn VL representations using transformers that take only low-level visual and\\nacoustic inputs without the prior existence of written language. The challenge lies in the difference\\nbetween text and acoustic signals; text is discrete and dense in information, while acoustic signals\\nare continuous and sparse in information [ 26;7]. Therefore, modality-speciﬁc architectures have\\nbeen used to model data from different modalities. It is only recently that researchers started using\\nmodality-agnostic transformer architecture to learn representations of different unimodal [ 17;19;8],\\nvision-text [ 32;54], or vision-audio-text [ 2] data. However, to the best of our knowledge, no previous\\nwork has explored a single homogeneous (modality-agnostic) minimalist transformer that learns\\nvisual-linguistic representations directly from visual and acoustic input at the perception level (without\\nrelying on text), and also makes the textless VL model more compact and efﬁcient than the existing\\ntext-based VL models (see Sec. 2 for details).\\n\\x03equal contribution\\n1Our code and checkpoints are available at: https://github.com/zinengtang/TVLT\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2209.14156v2  [cs.CV]  2 Nov 2022'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_one= pdfdoc.getPage(0)\n",
    "page_one.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages=[]\n",
    "pagesContent=[]\n",
    "for i in pdfdoc.pages:\n",
    "    pages.append((i))\n",
    "    pagesContent.append(i.extract_text())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVLT: Textless Vision-Language Transformer\n",
      "Zineng Tang\u0003Jaemin Cho\u0003Yixin Nie\u0003Mohit Bansal\n",
      "UNC Chapel Hill\n",
      "{terran, jmincho, yixin1, mbansal}@cs.unc.edu\n",
      "Abstract\n",
      "In this work, we present the Textless Vision-Language Transformer (TVLT), where\n",
      "homogeneous transformer blocks take raw visual and audio inputs for vision-and-\n",
      "language representation learning with minimal modality-speciﬁc design, and do\n",
      "not use text-speciﬁc modules such as tokenization or automatic speech recognition\n",
      "(ASR). TVLT is trained by reconstructing masked patches of continuous video\n",
      "frames and audio spectrograms (masked autoencoding) and contrastive modeling\n",
      "to align video and audio. TVLT attains performance comparable to its text-based\n",
      "counterpart on various multimodal tasks, such as visual question answering, image\n",
      "retrieval, video retrieval, and multimodal sentiment analysis, with 28x faster infer-\n",
      "ence speed and only 1/3 of the parameters. Our ﬁndings suggest the possibility\n",
      "of learning compact and efﬁcient visual-linguistic representations from low-level\n",
      "visual and audio signals without assuming the prior existence of text.1\n",
      "1 Introduction\n",
      "Humans perceive and learn the external world through signals from multiple modalities. To embody\n",
      "such human learning in machines, substantial research efforts are dedicated to developing vision-\n",
      "and-language (VL) models that can understand the joint semantics between visual and linguistic\n",
      "modalities and solve tasks such as visual question answering [ 4]. Although most such VL models use\n",
      "written language rather than spoken language as the main verbal communication channel, the default\n",
      "communication modality among humans has been speech, since circa 100,000 BCE [ 78]. Written\n",
      "language is relatively recent; cuneiform script, the earliest writing system, was developed circa 3,200\n",
      "BCE [ 65]. Moreover, we have witnessed an increasing usage of AI models in real-world products\n",
      "such as virtual assistants and smart speakers [ 40], where perception-level signals such as video and\n",
      "audio are the natural form of input. Intuitively, direct modeling of such signals will potentially yield\n",
      "more compact and efﬁcient representations.\n",
      "Transformers [ 81] have recently achieved great success in vision-language representation learning [ 76;\n",
      "10;48;74;87;86] by using text-based modules [ 15] on text-annotated images or videos. However,\n",
      "it is non-trivial to learn VL representations using transformers that take only low-level visual and\n",
      "acoustic inputs without the prior existence of written language. The challenge lies in the difference\n",
      "between text and acoustic signals; text is discrete and dense in information, while acoustic signals\n",
      "are continuous and sparse in information [ 26;7]. Therefore, modality-speciﬁc architectures have\n",
      "been used to model data from different modalities. It is only recently that researchers started using\n",
      "modality-agnostic transformer architecture to learn representations of different unimodal [ 17;19;8],\n",
      "vision-text [ 32;54], or vision-audio-text [ 2] data. However, to the best of our knowledge, no previous\n",
      "work has explored a single homogeneous (modality-agnostic) minimalist transformer that learns\n",
      "visual-linguistic representations directly from visual and acoustic input at the perception level (without\n",
      "relying on text), and also makes the textless VL model more compact and efﬁcient than the existing\n",
      "text-based VL models (see Sec. 2 for details).\n",
      "\u0003equal contribution\n",
      "1Our code and checkpoints are available at: https://github.com/zinengtang/TVLT\n",
      "36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2209.14156v2  [cs.CV]  2 Nov 2022\n"
     ]
    }
   ],
   "source": [
    "print(pagesContent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1 - \n",
      " SAMPLE  \n",
      "(All names and details provided in this sample are fictitious.   \n",
      "Some fields have been deliberately left blank. ) \n",
      " \n",
      " \n",
      "MEDICAL REPORT  \n",
      " \n",
      "SECTION 1: PATIENT’S PARTICULARS  \n",
      " \n",
      "Full name of patient:  Mr Tan Ah Kow  \n",
      " \n",
      "NRIC/FIN/Passport no. of patient:  S1111111X  \n",
      " \n",
      "Age of patient:  55 years old  \n",
      " \n",
      "SECTION 2: DOCTOR’S PARTICULARS  \n",
      " \n",
      "Full name of doctor:  Tan Ah Moi  \n",
      " \n",
      "NRIC/FIN/Passport no. of doctor:  S2222222Z   \n",
      " \n",
      " \n",
      "MCR no. of doctor:  333333  \n",
      " \n",
      "Hospital / Clinic name and address:  1 Blackacre Hospital, Singapore 01010101  \n",
      " \n",
      " \n",
      " \n",
      "Doctor’s qualifications and experience in this area of work:  \n",
      " \n",
      " \n",
      "[To set out details ] \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "f =  open(\"./pdf_to__doc/report.pdf\",'rb')\n",
    "pd = PyPDF2.PdfFileReader(f)\n",
    "print(pd.pages[0].extract_text())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
